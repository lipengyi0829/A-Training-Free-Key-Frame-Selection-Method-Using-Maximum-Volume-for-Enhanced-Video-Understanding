{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2ac22e38-3474-439e-b7f2-1ff17f8f00f2",
   "metadata": {},
   "source": [
    "# 1. Define  MaxInfo Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdf1cc03-0b65-4568-9e7f-a7068a569204",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "warning: fast C maxvol functions are not compiled, continue with python maxvol functions\n"
     ]
    }
   ],
   "source": [
    "# define svd \n",
    "\"\"\"\n",
    "r is the region of the slice\n",
    "list_of_feature is video -> images -> features, rows is frame's features\n",
    "\"\"\"\n",
    "def svd_reduce_dim(list_of_feature, r = 6):\n",
    "\n",
    "    u, s, v = np.linalg.svd(list_of_feature, full_matrices=False)\n",
    "    X = u\n",
    "    output = X[:, :r]\n",
    "     \n",
    "    return output\n",
    "\n",
    "# define max vol\n",
    "\"\"\"\n",
    "piv is select of key_frames index. \n",
    "initail_M is svd output.\n",
    "\"\"\"\n",
    "import numpy as np\n",
    "\n",
    "from maxvolpy.maxvolpy.maxvol import rect_maxvol\n",
    "\n",
    "def max_vol_extract_index(initail_M, tol, minK=1, maxK=500):\n",
    "    \n",
    "    piv, C = rect_maxvol(initail_M, tol = 0.1)\n",
    "    \n",
    "    print(np.allclose(initail_M, C.dot(initail_M[piv])))\n",
    "    \n",
    "    return piv\n",
    "\n",
    "import torch\n",
    "\n",
    "\n",
    "def reduce_video_frames(vision_model, vision_processor, video_input, max_frames=50):\n",
    "    \n",
    "    def auto_select_device():\n",
    "        if torch.cuda.is_available():\n",
    "            num_gpus = torch.cuda.device_count()\n",
    "            for i in range(num_gpus):\n",
    "                try:\n",
    "                    device = torch.device(f\"cuda:{i}\")\n",
    "                    with torch.cuda.device(device):\n",
    "                        pass\n",
    "                    return device \n",
    "                except:\n",
    "                    continue\n",
    "        return torch.device(\"cpu\")\n",
    "\n",
    "    device = auto_select_device()\n",
    "    \n",
    "    image_features = []\n",
    "    \n",
    "    for input in video_input:\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            inputs = vision_processor(images=input, return_tensors=\"pt\", padding=True).to(device)\n",
    "            image_feature = vision_model.get_image_features(**inputs)\n",
    "            image_feature = image_feature[0]\n",
    "\n",
    "        image_features.append(image_feature.cpu())\n",
    "\n",
    "    low_dimensions = svd_reduce_dim(image_features, r = 8\n",
    "\n",
    "    selections = max_vol_extract_index(low_dimensions, tol = 0.30)\n",
    "\n",
    "    selections.sort()\n",
    "    \n",
    "    filtered_frames = selections\n",
    "\n",
    "    if len(filtered_frames) > 1 and len(filtered_frames) % 2 != 0:\n",
    "        filtered_frames = filtered_frames[:-1]\n",
    "    \n",
    "    selected_pixels = video_input[filtered_frames]\n",
    "    \n",
    "    return selected_pixels\n",
    "\n",
    "def reduce_video_frames1(video_input, max_frames=768):\n",
    "    \"\"\"\n",
    "    Reduce the number of frames in a video tensor to a maximum of max_frames.\n",
    "    \n",
    "    Args:\n",
    "    video_input (torch.Tensor): Input video tensor of shape [num_frames, channels, height, width]\n",
    "    max_frames (int): Maximum number of frames in the output tensor\n",
    "    \n",
    "    Returns:\n",
    "    torch.Tensor: Video tensor with reduced number of frames\n",
    "    \"\"\"\n",
    "    num_frames, channels, height, width = video_input.shape\n",
    "    \n",
    "    if num_frames <= max_frames:\n",
    "        return video_input\n",
    "    \n",
    "    # Calculate indices of frames to keep\n",
    "    keep_indices = torch.linspace(0, num_frames - 1, max_frames).long()\n",
    "    \n",
    "    # Select frames\n",
    "    reduced_video = video_input[keep_indices]\n",
    "    \n",
    "    return reduced_video"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d88f360d-3a60-474c-9da6-14a396965277",
   "metadata": {},
   "source": [
    "# 2. Define Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "938100ac-04c5-4283-902f-c8ba5eb20995",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import math\n",
    "import json\n",
    "import argparse\n",
    "import warnings\n",
    "import traceback\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import Qwen2VLForConditionalGeneration, AutoTokenizer, AutoProcessor\n",
    "from transformers import AutoProcessor, CLIPModel\n",
    "from qwen_vl_utils import process_vision_info\n",
    "\n",
    "warnings.filterwarnings('ignore', category=UserWarning, message='TypedStorage is deprecated')\n",
    "\n",
    "def split_list(lst, n):\n",
    "    \"\"\"Split a list into n (roughly) equal-sized chunks\"\"\"\n",
    "    chunk_size = math.ceil(len(lst) / n)\n",
    "    return [lst[i:i+chunk_size] for i in range(0, len(lst), chunk_size)]\n",
    "\n",
    "def get_chunk(lst, n, k):\n",
    "    chunks = split_list(lst, n)\n",
    "    return chunks[k]\n",
    "class EgoschemaDataset(Dataset):\n",
    "    video_formats = ['.mp4', '.avi', '.mov', '.mkv']\n",
    "\n",
    "    def __init__(self, data_folder, data_list):\n",
    "        self.data_folder = data_folder\n",
    "        self.data_list = data_list\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_list)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        line = self.data_list[idx]\n",
    "        q_uid = line['q_uid']\n",
    "\n",
    "        for fmt in self.video_formats:\n",
    "            temp_path = os.path.join(self.data_folder, f\"{q_uid}{fmt}\")\n",
    "            if os.path.exists(temp_path):\n",
    "                video_path = temp_path\n",
    "                break\n",
    "\n",
    "        video_input = {\"type\": \"video\", \"video\": video_path, \"fps\": 2.0}\n",
    "        question = line['question']\n",
    "        a0, a1, a2, a3, a4 = line['option 0'], line['option 1'], line['option 2'], line['option 3'], line['option 4']\n",
    "        instruct = f'Question: {question}\\nOptions:\\n(A) {a0}\\n(B) {a1}\\n(C) {a2}\\n(D) {a3}\\n(E) {a4}\\nAnswer with the option\\'s letter from the given choices directly and only give the best option.'\n",
    "\n",
    "        return {\n",
    "            'q_uid': q_uid,\n",
    "            'video': video_input, \n",
    "            'instruct': instruct,\n",
    "        }\n",
    "\n",
    "def build_egoschema_eval(args):\n",
    "    questions = json.load(open(args.question_file, \"r\"))\n",
    "    questions = get_chunk(questions, args.num_chunks, args.chunk_idx)\n",
    "    dataset = EgoschemaDataset(args.video_folder, questions)\n",
    "    dataloader = DataLoader(dataset, batch_size=args.batch_size, shuffle=False, num_workers=args.num_workers)\n",
    "    return dataloader\n",
    "\n",
    "def convert_dict(orig_dict):\n",
    "    new_dict = {}\n",
    "    \n",
    "    # Process each item\n",
    "    new_dict['type'] = orig_dict['type'][0]\n",
    "    new_dict['video'] = orig_dict['video'][0]  # replace with the actual path if needed\n",
    "    new_dict['max_pixels'] = 360 * 420  # calculated from the context or could use the original value\n",
    "    new_dict['fps'] = float(orig_dict['fps'][0])  # Convert tensor to float\n",
    "    \n",
    "    return new_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3883be5b-9bea-458a-93f7-df2fdad7e24f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import argparse\n",
    "# pip install git+https://github.com/LLaVA-VL/LLaVA-NeXT.git\n",
    "from llava.model.builder import load_pretrained_model\n",
    "from llava.mm_utils import get_model_name_from_path, process_images, tokenizer_image_token\n",
    "from llava.constants import IMAGE_TOKEN_INDEX, DEFAULT_IMAGE_TOKEN, DEFAULT_IM_START_TOKEN, DEFAULT_IM_END_TOKEN, IGNORE_INDEX\n",
    "from llava.conversation import conv_templates, SeparatorStyle\n",
    "from PIL import Image\n",
    "import requests\n",
    "import copy\n",
    "import torch\n",
    "import sys\n",
    "import warnings\n",
    "from decord import VideoReader, cpu\n",
    "import numpy as np\n",
    "\n",
    "def load_video(video_path, max_frames_num,fps=1,force_sample=False):\n",
    "    if max_frames_num == 0:\n",
    "        return np.zeros((1, 336, 336, 3))\n",
    "    vr = VideoReader(video_path, ctx=cpu(0),num_threads=1)\n",
    "    total_frame_num = len(vr)\n",
    "    video_time = total_frame_num / vr.get_avg_fps()\n",
    "    fps = round(vr.get_avg_fps()/fps)\n",
    "    frame_idx = [i for i in range(0, len(vr), fps)]\n",
    "    frame_time = [i/fps for i in frame_idx]\n",
    "    if len(frame_idx) > max_frames_num or force_sample:\n",
    "        sample_fps = max_frames_num\n",
    "        uniform_sampled_frames = np.linspace(0, total_frame_num - 1, sample_fps, dtype=int)\n",
    "        frame_idx = uniform_sampled_frames.tolist()\n",
    "        frame_time = [i/vr.get_avg_fps() for i in frame_idx]\n",
    "    frame_time = \",\".join([f\"{i:.2f}s\" for i in frame_time])\n",
    "    spare_frames = vr.get_batch(frame_idx).asnumpy()\n",
    "    # import pdb;pdb.set_trace()\n",
    "    return spare_frames,frame_time,video_time\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33c535ad-66d0-4fb9-ac58-a5324bec41c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_inference(args):\n",
    "    \n",
    "    pretrained = args.model_path\n",
    "    model_name = args.model_name\n",
    "        \n",
    "    device = \"cuda\"\n",
    "    device_map = \"auto\"\n",
    "    tokenizer, model, processor, max_length = load_pretrained_model(pretrained, None, model_name, device_map=device_map)\n",
    "    model.eval()\n",
    "    \n",
    "    vision_model_path = \"openai/clip-vit-large-patch14-336\"\n",
    "    vision_model = CLIPModel.from_pretrained(vision_model_path, device_map=\"auto\")\n",
    "    vision_processor = AutoProcessor.from_pretrained(vision_model_path)\n",
    "\n",
    "    answer_file = os.path.expanduser(args.answer_file)\n",
    "    os.makedirs(os.path.dirname(answer_file), exist_ok=True)\n",
    "    ans_file = open(answer_file, \"w\")\n",
    "\n",
    "    val_loader = build_egoschema_eval(args)\n",
    "    frames = []\n",
    "\n",
    "    for batch in tqdm(val_loader):\n",
    " \n",
    "        q_uid = batch['q_uid'][0] if isinstance(batch['q_uid'], list) else batch['q_uid']\n",
    "        \n",
    "        instruct = batch['instruct'][0] if isinstance(batch['instruct'], list) else batch['instruct']\n",
    "\n",
    "        video = convert_dict(batch['video'])\n",
    "\n",
    "        try:\n",
    "\n",
    "            max_frames_num = 128\n",
    "        \n",
    "            video,frame_time,video_time = load_video(video['video'], max_frames_num, 1, force_sample=True)\n",
    "            \n",
    "            reduced_video_inputs = reduce_video_frames(vision_model, vision_processor, video)\n",
    "\n",
    "            reduced_video_inputs = reduce_video_frames1(reduced_video_inputs, 64)\n",
    "            \n",
    "            frames_str = len(reduced_video_inputs)\n",
    "\n",
    "            frames.append(frames_str)\n",
    "                \n",
    "            video = processor.preprocess(reduced_video_inputs, return_tensors=\"pt\")[\"pixel_values\"].cuda().half()\n",
    "            \n",
    "            video = [video]\n",
    "                    \n",
    "            conv_template = \"qwen_1_5\"  # Make sure you use correct chat template for different models\n",
    "            # time_instruciton = f\"The video lasts for {video_time:.2f} seconds, and {len(video[0])} frames are uniformly sampled from it. These frames are located at {frame_time}.Please answer the following questions related to this video.\"\n",
    "                    \n",
    "            question = DEFAULT_IMAGE_TOKEN + instruct\n",
    "                    \n",
    "            conv = copy.deepcopy(conv_templates[conv_template])\n",
    "                    \n",
    "            conv.append_message(conv.roles[0], question)\n",
    "                    \n",
    "            conv.append_message(conv.roles[1], None)\n",
    "                    \n",
    "            prompt_question = conv.get_prompt()\n",
    "                    \n",
    "            input_ids = tokenizer_image_token(prompt_question, tokenizer, IMAGE_TOKEN_INDEX, return_tensors=\"pt\").unsqueeze(0).to(device)\n",
    "                    \n",
    "            cont = model.generate(\n",
    "                input_ids,\n",
    "                images=video,\n",
    "                modalities= [\"video\"],\n",
    "                do_sample=False,\n",
    "                temperature=0,\n",
    "                max_new_tokens=4096,\n",
    "            )\n",
    "\n",
    "            output = tokenizer.batch_decode(cont, skip_special_tokens=True)[0].strip().rstrip(\".\")\n",
    "            \n",
    "            print(output)\n",
    "            \n",
    "            # Prepare a dictionary for egoschema_dump\n",
    "            result = {\n",
    "                'q_uid': q_uid,\n",
    "                'instruct': instruct,\n",
    "                'frames_str': frames_str\n",
    "            }\n",
    "\n",
    "            print(result['frames_str'])\n",
    "            \n",
    "            egoschema_dump(ans_file, result, output)\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing q_uid {q_uid}: {str(e)}\")\n",
    "            ans_file.write(f'{q_uid}, -1\\n')\n",
    "\n",
    "    ans_file.close()\n",
    "\n",
    "    file_path = 'frames-informations'\n",
    "    \n",
    "    with open(file_path, 'w') as json_file:\n",
    "        json.dump(frames, json_file)\n",
    "\n",
    "def egoschema_dump(ans_file, line, output):\n",
    "    q_uid = line['q_uid']\n",
    "    letters = ['A', 'B', 'C', 'D', 'E']\n",
    "\n",
    "    pred_answer = re.findall('[\\(\\ ]*[A-E][\\)\\ ]*', output)\n",
    "    try:\n",
    "        if len(pred_answer) >= 1:\n",
    "            pred_answer = pred_answer[0].strip()\n",
    "            pred_answer = pred_answer.strip('()')\n",
    "            pred_idx = letters.index(pred_answer)\n",
    "        else:\n",
    "            print(f'The video \"{q_uid}\" output \"{output}\" is not in the expected format')\n",
    "            pred_idx = -1  # or some default value\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing output for q_uid {q_uid}: {str(e)}\")\n",
    "        pred_idx = -1  # or some default value\n",
    "\n",
    "    ans_file.write(f'{q_uid}, {pred_idx}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99eac70a-5ad7-4c97-91df-8168410162d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "sys.argv = ['ipykernel_launcher.py', \n",
    "            '--model-path', 'lmms-lab/LLaVA-Video-72B-Qwen2',\n",
    "            '--model-name', 'llava_qwen',\n",
    "            '--video-folder', 'egoschema/videos',\n",
    "            '--question-file', 'egoschema/questions.json']\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--model-path', required=True)\n",
    "parser.add_argument('--video-folder', required=True)\n",
    "parser.add_argument('--question-file', required=True)\n",
    "parser.add_argument('--answer-file', required=True)\n",
    "parser.add_argument(\"--num-chunks\", type=int, default=1)\n",
    "parser.add_argument(\"--chunk-idx\", type=int, default=0)\n",
    "parser.add_argument(\"--device\", type=str, required=False, default='cuda:0')\n",
    "parser.add_argument(\"--batch-size\", type=int, default=1)\n",
    "parser.add_argument(\"--num-workers\", type=int, default=8)\n",
    "args = parser.parse_args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2892ec17-9d9e-4361-9f89-ef44b447ff96",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_inference(args)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mllm",
   "language": "python",
   "name": "mllm"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
