{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67561183-1f60-41dc-938a-7e71b7fcf7d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" \n",
    "r is the region of the slice\n",
    "list_of_feature is video -> images -> features, rows is frame's features\n",
    "\"\"\"\n",
    "def svd_reduce_dim(list_of_feature, r = 6):\n",
    "\n",
    "    u, s, v = np.linalg.svd(list_of_feature, full_matrices=False)\n",
    "    X = u\n",
    "    output = X[:, :r]\n",
    "     \n",
    "    # a matrix can directly to use maxvol\n",
    "    return output\n",
    "\n",
    "\"\"\"\n",
    "piv is select of key_frames index. \n",
    "initail_M is svd output.\n",
    "\"\"\"\n",
    "import numpy as np\n",
    "\n",
    "from maxvolpy.maxvolpy.maxvol import rect_maxvol\n",
    "\n",
    "def max_vol_extract_index(initail_M, tol, minK=1, maxK=500):\n",
    "    # piv, C = rect_maxvol(u, tol = 0.1, minK, maxK)\n",
    "    piv, C = rect_maxvol(initail_M, tol = 0.1)\n",
    "    \n",
    "    print(np.allclose(initail_M, C.dot(initail_M[piv])))\n",
    "    \n",
    "    return piv\n",
    "\n",
    "def reduce_video_frames(vision_model, vision_processor, video_input, max_frames=50):\n",
    "    \n",
    "    def auto_select_device():\n",
    "        if torch.cuda.is_available():\n",
    "            num_gpus = torch.cuda.device_count()\n",
    "            for i in range(num_gpus):\n",
    "                try:\n",
    "                    device = torch.device(f\"cuda:{i}\")\n",
    "                    with torch.cuda.device(device):\n",
    "                        pass\n",
    "                    return device \n",
    "                except:\n",
    "                    continue\n",
    "        return torch.device(\"cpu\")\n",
    "\n",
    "    device = auto_select_device()\n",
    "    \n",
    "    image_features = []\n",
    "    \n",
    "    for input in video_input:\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            inputs = vision_processor(images=input, return_tensors=\"pt\", padding=True).to(device)\n",
    "            image_feature = vision_model.get_image_features(**inputs)\n",
    "            image_feature = image_feature[0]\n",
    "\n",
    "        image_features.append(image_feature.cpu())\n",
    "\n",
    "    low_dimensions = svd_reduce_dim(image_features, r = 8\n",
    "\n",
    "    selections = max_vol_extract_index(low_dimensions, tol = 0.30)\n",
    "\n",
    "    selections.sort()\n",
    "    \n",
    "    filtered_frames = selections\n",
    "\n",
    "    if len(filtered_frames) > 1 and len(filtered_frames) % 2 != 0:\n",
    "        filtered_frames = filtered_frames[:-1]\n",
    "    \n",
    "    selected_pixels = video_input[filtered_frames]\n",
    "    \n",
    "    return selected_pixels\n",
    "\n",
    "def reduce_video_frames1(video_input, max_frames=768):\n",
    "    \"\"\"\n",
    "    Reduce the number of frames in a video tensor to a maximum of max_frames.\n",
    "    \n",
    "    Args:\n",
    "    video_input (torch.Tensor): Input video tensor of shape [num_frames, channels, height, width]\n",
    "    max_frames (int): Maximum number of frames in the output tensor\n",
    "    \n",
    "    Returns:\n",
    "    torch.Tensor: Video tensor with reduced number of frames\n",
    "    \"\"\"\n",
    "    num_frames, channels, height, width = video_input.shape\n",
    "    \n",
    "    if num_frames <= max_frames:\n",
    "        return video_input\n",
    "    \n",
    "    # Calculate indices of frames to keep\n",
    "    keep_indices = torch.linspace(0, num_frames - 1, max_frames).long()\n",
    "    \n",
    "    # Select frames\n",
    "    reduced_video = video_input[keep_indices]\n",
    "    \n",
    "    return reduced_video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3bccdbc-575a-4c1b-ad8f-0d00d2753109",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import math\n",
    "import json\n",
    "import argparse\n",
    "import warnings\n",
    "import traceback\n",
    "\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from transformers import Qwen2VLForConditionalGeneration, AutoTokenizer, AutoProcessor\n",
    "from qwen_vl_utils import process_vision_info\n",
    "\n",
    "warnings.filterwarnings('ignore', category=UserWarning, message='TypedStorage is deprecated')\n",
    "\n",
    "def split_list(lst, n):\n",
    "    \"\"\"Split a list into n (roughly) equal-sized chunks\"\"\"\n",
    "    chunk_size = math.ceil(len(lst) / n)  # integer division\n",
    "    return [lst[i:i+chunk_size] for i in range(0, len(lst), chunk_size)]\n",
    "\n",
    "def get_chunk(lst, n, k):\n",
    "    chunks = split_list(lst, n)\n",
    "    return chunks[k]\n",
    "class EgoschemaDataset(Dataset):\n",
    "    video_formats = ['.mp4', '.avi', '.mov', '.mkv']\n",
    "\n",
    "    def __init__(self, data_folder, data_list):\n",
    "        self.data_folder = data_folder\n",
    "        self.data_list = data_list\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_list)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        line = self.data_list[idx]\n",
    "        q_uid = line['q_uid']  # Ensure this is correctly accessed from the data\n",
    "\n",
    "        for fmt in self.video_formats:\n",
    "            temp_path = os.path.join(self.data_folder, f\"{q_uid}{fmt}\")\n",
    "            if os.path.exists(temp_path):\n",
    "                video_path = temp_path\n",
    "                break\n",
    "\n",
    "        video_input = {\"type\": \"video\", \"video\": video_path, \"max_pixels\": 360 * 420, \"fps\": 3.0}\n",
    "\n",
    "        question = line['question']\n",
    "        a0, a1, a2, a3, a4 = line['option 0'], line['option 1'], line['option 2'], line['option 3'], line['option 4']\n",
    "        instruct = f'Question: {question}\\nOptions:\\n(A) {a0}\\n(B) {a1}\\n(C) {a2}\\n(D) {a3}\\n(E) {a4}\\nAnswer with the option\\'s letter from the given choices directly and only give the best option.'\n",
    "\n",
    "        return {\n",
    "            'q_uid': q_uid,\n",
    "            'video': video_input, \n",
    "            'instruct': instruct,\n",
    "        }\n",
    "\n",
    "def build_egoschema_eval(args):\n",
    "    questions = json.load(open(args.question_file, \"r\"))\n",
    "    questions = get_chunk(questions, args.num_chunks, args.chunk_idx)\n",
    "    dataset = EgoschemaDataset(args.video_folder, questions)\n",
    "    dataloader = DataLoader(dataset, batch_size=args.batch_size, shuffle=False, num_workers=args.num_workers)\n",
    "    return dataloader\n",
    "\n",
    "def convert_dict(orig_dict):\n",
    "    # Initialize a new dictionary\n",
    "    new_dict = {}\n",
    "    \n",
    "    # Process each item\n",
    "    new_dict['type'] = orig_dict['type'][0]\n",
    "    new_dict['video'] = orig_dict['video'][0]  # replace with the actual path if needed\n",
    "    new_dict['max_pixels'] = 360 * 420  # calculated from the context or could use the original value\n",
    "    new_dict['fps'] = float(orig_dict['fps'][0])  # Convert tensor to float\n",
    "    \n",
    "    return new_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5572147-040f-4b70-8321-c1478aab0986",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_inference(args):\n",
    "    \n",
    "    model = Qwen2VLForConditionalGeneration.from_pretrained(\n",
    "        args.model_path,\n",
    "        torch_dtype=\"auto\",\n",
    "        attn_implementation=\"flash_attention_2\",\n",
    "        device_map=\"auto\",\n",
    "    )\n",
    "    processor = AutoProcessor.from_pretrained(args.model_path)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(args.model_path)\n",
    "\n",
    "    # 7. define encoder --- torch_dtype=\"auto\", attn_implementation=\"flash_attention_2\",\n",
    "    vision_model_path = \"openai/clip-vit-large-patch14-336\"\n",
    "    vision_model = CLIPModel.from_pretrained(vision_model_path, device_map=\"auto\")\n",
    "    vision_processor = AutoProcessor.from_pretrained(vision_model_path)\n",
    "\n",
    "    answer_file = os.path.expanduser(args.answer_file)\n",
    "    os.makedirs(os.path.dirname(answer_file), exist_ok=True)\n",
    "    ans_file = open(answer_file, \"w\")\n",
    "\n",
    "    frames = []\n",
    "\n",
    "    val_loader = build_egoschema_eval(args)\n",
    "\n",
    "    for batch in tqdm(val_loader):\n",
    "\n",
    "        # q_uid: 004a7f7e-9e83-431f-bc98-859cf9024e93\n",
    "        \n",
    "        # instruct\n",
    "        # Question: What are the main ingredients and tools used during the video, and how do they contribute to the goal of the activity?\n",
    "        # Options:\n",
    "        # (A) The primary ingredients utilized in the demonstration video include peas, water, salt, along with a knife.\n",
    "        # (B) The main ingredients used in the video are peas, water, salt, and a fork.\n",
    "        # (C) The main ingredients used in the video are peas, water, and salt. the main tools used are a measuring cup, a pan, and a spoon.\n",
    "        # (D) In the video, the main ingredients utilized are peas, water, salt, and a plate to hold them.\n",
    "        # (E) In the instructional video, the primary ingredients employed are peas, water, salt, and a simple bowl for preparation.\n",
    "        # Answer with the option's letter from the given choices directly and only give the best option.\n",
    "        \n",
    "        q_uid = batch['q_uid'][0] if isinstance(batch['q_uid'], list) else batch['q_uid']\n",
    "        \n",
    "        instruct = batch['instruct'][0] if isinstance(batch['instruct'], list) else batch['instruct']\n",
    "\n",
    "        video = convert_dict(batch['video'])\n",
    "        \n",
    "        # Note: We're not using 'video' key as it's not present in the batch\n",
    "        messages = [\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": [\n",
    "                        video,\n",
    "                        {\"type\": \"text\", \"text\": instruct},\n",
    "                    ],\n",
    "                }\n",
    "            ]\n",
    "\n",
    "        try:\n",
    "            text = processor.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "            \n",
    "            image_inputs, video_inputs = process_vision_info(messages)\n",
    "\n",
    "            print(len(video_inputs[0]))\n",
    "            \n",
    "            # reduce \n",
    "            reduced_video_inputs = [reduce_video_frames(vision_model, vision_processor, vi) for vi in video_inputs]\n",
    "\n",
    "            reduced_video_inputs = reduce_video_frames1(reduced_video_inputs, 180)\n",
    "\n",
    "            print(len(reduced_video_inputs[0]))\n",
    "\n",
    "            frames.append(len(reduced_video_inputs[0]))\n",
    "            \n",
    "            inputs = processor(\n",
    "                text=[text],\n",
    "                images = image_inputs,\n",
    "                videos = reduced_video_inputs,\n",
    "                padding=True,\n",
    "                return_tensors=\"pt\",\n",
    "            )\n",
    "            inputs = inputs.to(model.device)\n",
    "\n",
    "            generated_ids = model.generate(**inputs, max_new_tokens=128)\n",
    "            generated_ids_trimmed = [out_ids[len(in_ids):] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)]\n",
    "            output = processor.batch_decode(generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\n",
    "\n",
    "            # Prepare a dictionary for egoschema_dump\n",
    "            result = {\n",
    "                'q_uid': q_uid,\n",
    "                'instruct': instruct\n",
    "            }\n",
    "            egoschema_dump(ans_file, result, output)\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing q_uid {q_uid}: {str(e)}\")\n",
    "            # Write a default answer or skip this question\n",
    "            ans_file.write(f'{q_uid}, -1\\n')\n",
    "\n",
    "    ans_file.close()\n",
    "\n",
    "    # 指定JSON文件的保存路径\n",
    "    file_path = 'frames'\n",
    "    \n",
    "    # 将数据写入JSON文件\n",
    "    with open(file_path, 'w') as json_file:\n",
    "        json.dump(frames, json_file)\n",
    "\n",
    "def egoschema_dump(ans_file, line, output):\n",
    "    q_uid = line['q_uid']\n",
    "    letters = ['A', 'B', 'C', 'D', 'E']\n",
    "\n",
    "    pred_answer = re.findall('[\\(\\ ]*[A-E][\\)\\ ]*', output)\n",
    "    try:\n",
    "        if len(pred_answer) >= 1:\n",
    "            pred_answer = pred_answer[0].strip()\n",
    "            pred_answer = pred_answer.strip('()')\n",
    "            pred_idx = letters.index(pred_answer)\n",
    "        else:\n",
    "            print(f'The video \"{q_uid}\" output \"{output}\" is not in the expected format')\n",
    "            pred_idx = -1  # or some default value\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing output for q_uid {q_uid}: {str(e)}\")\n",
    "        pred_idx = -1  # or some default value\n",
    "\n",
    "    ans_file.write(f'{q_uid}, {pred_idx}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d94c81e7-74fc-4950-84fa-2bdd8e48645a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import argparse\n",
    "\n",
    "# 模拟命令行参数\n",
    "sys.argv = ['ipykernel_launcher.py', \n",
    "            '--model-path', 'Qwen/Qwen2-VL-2B-Instruct',\n",
    "            '--video-folder', 'egoschema/videos',\n",
    "            '--question-file', 'egoschema/questions.json',\n",
    "            '--answer-file', 'egoschema/answer-2B-new-set.json']\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--model-path', required=True)\n",
    "parser.add_argument('--video-folder', required=True)\n",
    "parser.add_argument('--question-file', required=True)\n",
    "parser.add_argument('--answer-file', required=True)\n",
    "parser.add_argument(\"--num-chunks\", type=int, default=1)\n",
    "parser.add_argument(\"--chunk-idx\", type=int, default=0)\n",
    "parser.add_argument(\"--device\", type=str, required=False, default='cuda:0')\n",
    "parser.add_argument(\"--batch-size\", type=int, default=1)\n",
    "parser.add_argument(\"--num-workers\", type=int, default=8)\n",
    "args = parser.parse_args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77247429-3619-4f3e-af94-dd1e67ec8403",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mllm",
   "language": "python",
   "name": "mllm"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
